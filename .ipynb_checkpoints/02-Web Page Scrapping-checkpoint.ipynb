{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# urlib 임포트\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# https://news.daum.net/\n",
    "req = urlopen(\"https://news.daum.net/\")\n",
    "# print(\"응답 코드:\", req.getcode())  # 200 : 성공\n",
    "\n",
    "if req.getcode() == 200:\n",
    "    # 성공\n",
    "    # html 받아오기\n",
    "    html = req.read()\n",
    "    html = html.decode(\"utf-8\")\n",
    "    print(\"SUCCESS\")\n",
    "else:\n",
    "    # 실패\n",
    "    print(\"HTTP-ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text -> html dom으로 변경\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "# print(soup.prettify())\n",
    "# 문서 정보의 확인\n",
    "print(\"문서 제목:\", soup.title.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 주요 기사 div를 검색\n",
    "# select or find\n",
    "articles = soup.select(\"#mArticle\")[0]  # id=mArticle\n",
    "# print(type(articles))  # 여러 개 있음.. index가 0번인것부터 봐보자.\n",
    "# print(articles.prettify())\n",
    "# div#mArticle 자손인 a.link_txt를 검색\n",
    "links = articles.select(\"a.link_txt\")  # class=link_txt인 a 태그\n",
    "# print(links)  # 여러 개 있음..\n",
    "# print(links[0])\n",
    "# 기사 목록을 { \"link\": ..., \"title\":...} 의 리스트\n",
    "\n",
    "news_list = []\n",
    "\n",
    "for link in links:\n",
    "#     print(link)\n",
    "    news = {\n",
    "        \"link\": link['href'],  # 속성에 접근\n",
    "        \"title\": link.text.strip()\n",
    "    }\n",
    "    news_list.append(news)\n",
    "# print(news_list)\n",
    "\n",
    "print(\"다음 뉴스 주요 기사:\")\n",
    "for news in news_list:\n",
    "    print(\"{} - {}\".format(news.get('title'), news.get('link')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 연습\n",
    "# 다음 뉴스 상단 div#cSub인 뉴스 수집"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 네이버 영화\n",
    "# https://movie.naver.com/movie/running/current.nhn\n",
    "# 현재 상영영화 순위를 수집\n",
    "req = urlopen(\"https://movie.naver.com/movie/running/current.nhn\")\n",
    "html_str = req.read().decode('utf-8')\n",
    "# print(html_str[:256])  # html text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DOM 변환\n",
    "soup = BeautifulSoup(html_str, \"html.parser\")\n",
    "# print(soup.body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ul.lst_detail_t1\n",
    "current_movies = soup.select(\"ul.lst_detail_t1 > li\") \n",
    "# class=lst_detail_t1인 ul의 자식인 li 태그들\n",
    "# print(current_movies)\n",
    "\n",
    "# 영화 세부 정보 검색\n",
    "movie_list = []\n",
    "\n",
    "for movie in current_movies: # li\n",
    "    titles = movie.select(\"dl.lst_dsc > .tit > a\")[0]\n",
    "    \n",
    "    info = {\"title\": titles.text.strip(), \"link\": titles['href']}\n",
    "    movie_list.append(info)\n",
    "    \n",
    "# print(movie_list)\n",
    "print(\"네이버 영화 순위\")\n",
    "\n",
    "for rank, info in enumerate(movie_list):\n",
    "    print(\"{}위 - {} : {}\".format(rank+1, info['title'], info['link']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이미지 다운로드\n",
    "# ul.lst_detail_t1 > li -> img\n",
    "# img 태그의 속성 src, alt(대체 텍스트)\n",
    "\n",
    "import os\n",
    "import re  # 정규표현식\n",
    "from urllib.request import urlretrieve  # 네이버 영화 > 이미지 다운로드\n",
    "\n",
    "if not os.path.exists(\"d:/data-science/crawl\"):  # 디렉터리 없으면\n",
    "    os.makedirs(\"d:/data-science/crawl\")  # 생성\n",
    "\n",
    "for movie in current_movies:\n",
    "    img_tag = movie.select(\"img\")[0]\n",
    "#     print(\"포스터:\", img_tag)\n",
    "#     print(\"src:\", img_tag['src'])\n",
    "#     print(\"alt:\", img_tag['alt'])\n",
    "    # 파일명으로 사용할 수 없는 문자를 제거\n",
    "    title = img_tag['alt']\n",
    "    title = re.sub(r\"[\\/:*?<>|.]\", \"-\",  title.strip())  # \\/:*?<>|. 이런 문자들이 나오면 -로 바꿔줘라\n",
    "#     print(\"정제된 제목:\", title)\n",
    "    src = img_tag['src']  # 이미지 주소\n",
    "    target = \"d:/data-science/crawl/{}.jpg\".format(title)\n",
    "    \n",
    "    # 이미지 다운로드 : urlretrieve\n",
    "    urlretrieve(src, target)\n",
    "    print(\".\", end=\"\")\n",
    "else:\n",
    "    print(\"Download Complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 연습문제\n",
    "# https://movie.naver.com/movie/running/premovie.nhn\n",
    "# 개봉 예정 영화 정보를 수집해 봅시다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
